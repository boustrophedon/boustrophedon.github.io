<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">

    <title>CPU caches and GPU shared memory &ndash; recycled math</title>

    <!-- Meta -->
    <meta name="description" content="recycled math &ndash; ">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Social -->
    <meta property="article:author" content="Harry Stern" />
    <meta property="article:section" content="drafts" />
    <meta property="article:published_time" content="2018-10-15" />

    <meta property="og:type" content="article"/>
    <meta property="og:title" content="CPU caches and GPU shared memory"/>
    <meta property="og:description" content="I was watching a Scott Meyers talk on CPU caches and towards the very end I realized something: Managing per-block shared memory is simply manually managing a &#34;L4&#34; CPU cache. Linear memory access patterns are coalesced such that it mimics the effect of linearly traversing data in a cache line â€¦"/>
    <meta property="og:site_name" content="recycled math" />
    <meta property="og:url" content="https://harrystern.net/drafts/cpu-caches-and-gpu-shared-memory.html"/>


    <!-- Feed -->
    <link rel="alternate" type="application/atom+xml" href="https://harrystern.net/feeds/all.atom.xml" title="recycled math Atom Feed" />
    <link rel="alternate" type="application/rss+xml" href="https://harrystern.net/feeds/all.rss.xml" title="recycled math RSS Feed" />

    <!-- CSS -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Open+Sans:regular,bold&display=swap">
    <link rel="stylesheet" type="text/css" href="https://harrystern.net/theme/css/style.css">
    <link rel="stylesheet" type="text/css" href="https://harrystern.net/theme/css/pygments-highlight-github.css">

    <!-- Icon -->

  </head>

  <body>
    <div class="header-box">
      <header id="header">
	    <a href="https://harrystern.net" id="site-title"> recycled math </a>
        <nav id="header-menu">
          <ul>
            <li><a href="/about.html">About</a></li>
            <li><a href="/archive.html">Archive</a></li>
          </ul>
        </nav>
      </header>
    </div>



    <br><br>

    <article>
      <header class="col-main">
        <h1>CPU caches and GPU shared memory</h1>
        <div class="post-info">
          <div class="" style="flex-grow: 1;">
            <span><time datetime="2018-10-15T00:00:00-04:00">Mon 15 October 2018</time> in <a href="https://harrystern.net/category/drafts.html" title="All articles in category drafts">drafts</a></span>
   	  <span> - 1 min </span>
          </div>
        </div>
      </header>

      <br>


      <div class="col-main">
        <section id="content">
          <p>I was watching <a href="https://www.youtube.com/watch?v=WDIkqP4JbkE">a Scott Meyers talk on CPU caches</a> and towards the very end I realized something: Managing per-block shared memory is simply manually managing a "L4" CPU cache. Linear memory access patterns are <a href="https://cvw.cac.cornell.edu/gpu/coalesced">coalesced</a> such that it mimics the effect of linearly traversing data in a cache line (and in fact, Meyers mentions that strided memory access is detected by modern CPUs and this is also the case for coalesced memory access), but at the end he also fielded a question about whether we can "pin cache lines", and the answer being kind of but you really don't want to.</p>
<p>However, in GPU programming, this is effectively what shared memory is: in a given thread block, you perform a main memory access, load it into a cache, and keep it there for a given duration specifically to reduce latency when accessing that same information again. In fact, <a href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g6c9cc78ca80490386cf593b4baa35a15">there are functions</a> that allow you to dynamically partition the size of the L1 cache and available shared memory for some (older, it seems) GPUs.  </p>
<p>In fact, the CUDA best practices guide <a href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c-ab">specifically says</a> "This illustrates the use of the shared memory as a user-managed cache when the hardware L1 cache eviction policy does not match up well with the needs of the application or when L1 cache is not used for reads from global memory."</p>
        </section>

      </div>
    </article>


    <footer id="footer">
      <div id="footer-copyright" class="">
        <span>
          &copy;
          2014&dash;2021          Harry Stern
 | <a href="https://harrystern.net/feeds/all.atom.xml">Atom feed</a> | <a href="https://harrystern.net/feeds/all.rss.xml">RSS feed</a>        </span>
      </div>
    </footer>

  </body>
</html>